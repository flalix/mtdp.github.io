[
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "The Molecular Target Discovery Pipeline (MTDp)\n\n\nMTDp is for labs that utilizaes gene expression omics to find pahtways and targets.\n\n\n\n\n\n\n\n\nThe current version only uses Reactome to calculate the geneset enrichemnt analysis (GSEA)\nDPC uses Gemini models 1.5-pro and 1.5-flash, but will be generalized soon.\n\n\n\nThe current version was developed and tested in Linux\n\n\n\nThere is no web site for the moment, bioinformatians can download/fork the current version and run it in Jupygter notebook, after configuring its own environement.\n\n\n\n\nThe Molecular Target Discovery Pipeline comprises on the following sub-pipelines:\n\nBest Cutoff Algorithm (BCA)\n1.1. scan the LFC-FDR landscape\n1.2. allow the user to chose the ‚Äòbest‚Äô parameters\n1.3. recalculates DEGs/DAPs and Enriched Pathways\n\nDigital Pathway Curation (DPC) pipeline\n2.1. The Ensemble model\n2.2. The 3 Sources model\n2.3. Crowdsource-consensus (CSC) & accuracies\n\npseudo-Pathway modulation (pPMod)\n3.1. Calculates the pPM inddex\n3.2. Plot the main pseudo-modulated pathways\n- Pathway Heatmaps\n- Gene/Protein heatmaps\n3.3. Plot the LFC-path\n\nMolecular Target in silico Validation\n4.1. Read & Classify the literature\n4.2. Find the Genes present in the literature\n4.3. Order the Genes\n4.4. Uses cMAP to validate genes and to find Mechanism of Action (MOA)\n\n\n\n\nThe first algorithm/pipeline is BCA, and it analises if the default cutoff parameters (LFC and FDR for gene expression and FDR for geneset enrichment analysis) are correct and appropiate, offering and Landscape allowing the user to change them; therefore, changing the parameters, DEGs/DAPs changes, following by new enriched pathways.\nThe\n\n\n\nAll code was written in Python 3.11, besides small codes in shell script. The best environment to run is Linux or Linux-like environment.\n\n\n\nMTP was developed by PhD Flavio Lichtenstein at CENTD - Instituto Butantan - S√£o Paulo, Brazil email: flavio.lichtenstein@butantan.gov.br or flalix@gmail.com\n\n\n\nTo run your own data one must:\n\nDowload the whole project.\nCreate the enviroment with Conda or similar framework, accordingo to requirments.\nDefine your projec.yml parameters 3.1 define the disease 3.2 define cases / subtypes 3.3 rename accordingly the LFC tables\n\n\n\n\n\n\n\nTip\n\n\n\nSee: configuring your own environment\n\n\n\n\nAll presentd data belongs to: 1. COVID-19 - proteomics - PhD Sonia Aparecida de Andrade at Escola Superior, Butantan Institute, S√£o Paulo, SP, Brazil 2. Medullobalstoma - microarray - PhD Aparecida Maria Fontes at Department of Genetics, Ribeir√£o Preto Medical School, University of S√£o Paulo, Ribeir√£o Preto\n\n\n\n\nThis book is free and open source, so please do open an issue if you notice a problem!"
  },
  {
    "objectID": "index.html#who-is-this-for",
    "href": "index.html#who-is-this-for",
    "title": "Preface",
    "section": "",
    "text": "MTDp is for labs that utilizaes gene expression omics to find pahtways and targets."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "Preface",
    "section": "",
    "text": "The current version only uses Reactome to calculate the geneset enrichemnt analysis (GSEA)\nDPC uses Gemini models 1.5-pro and 1.5-flash, but will be generalized soon.\n\n\n\nThe current version was developed and tested in Linux\n\n\n\nThere is no web site for the moment, bioinformatians can download/fork the current version and run it in Jupygter notebook, after configuring its own environement."
  },
  {
    "objectID": "index.html#the-pipeline",
    "href": "index.html#the-pipeline",
    "title": "Preface",
    "section": "",
    "text": "The Molecular Target Discovery Pipeline comprises on the following sub-pipelines:\n\nBest Cutoff Algorithm (BCA)\n1.1. scan the LFC-FDR landscape\n1.2. allow the user to chose the ‚Äòbest‚Äô parameters\n1.3. recalculates DEGs/DAPs and Enriched Pathways\n\nDigital Pathway Curation (DPC) pipeline\n2.1. The Ensemble model\n2.2. The 3 Sources model\n2.3. Crowdsource-consensus (CSC) & accuracies\n\npseudo-Pathway modulation (pPMod)\n3.1. Calculates the pPM inddex\n3.2. Plot the main pseudo-modulated pathways\n- Pathway Heatmaps\n- Gene/Protein heatmaps\n3.3. Plot the LFC-path\n\nMolecular Target in silico Validation\n4.1. Read & Classify the literature\n4.2. Find the Genes present in the literature\n4.3. Order the Genes\n4.4. Uses cMAP to validate genes and to find Mechanism of Action (MOA)"
  },
  {
    "objectID": "index.html#sub-pipelines",
    "href": "index.html#sub-pipelines",
    "title": "Preface",
    "section": "",
    "text": "The first algorithm/pipeline is BCA, and it analises if the default cutoff parameters (LFC and FDR for gene expression and FDR for geneset enrichment analysis) are correct and appropiate, offering and Landscape allowing the user to change them; therefore, changing the parameters, DEGs/DAPs changes, following by new enriched pathways.\nThe"
  },
  {
    "objectID": "index.html#code-and-environment",
    "href": "index.html#code-and-environment",
    "title": "Preface",
    "section": "",
    "text": "All code was written in Python 3.11, besides small codes in shell script. The best environment to run is Linux or Linux-like environment."
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Preface",
    "section": "",
    "text": "MTP was developed by PhD Flavio Lichtenstein at CENTD - Instituto Butantan - S√£o Paulo, Brazil email: flavio.lichtenstein@butantan.gov.br or flalix@gmail.com"
  },
  {
    "objectID": "index.html#running-the-code-yourself",
    "href": "index.html#running-the-code-yourself",
    "title": "Preface",
    "section": "",
    "text": "To run your own data one must:\n\nDowload the whole project.\nCreate the enviroment with Conda or similar framework, accordingo to requirments.\nDefine your projec.yml parameters 3.1 define the disease 3.2 define cases / subtypes 3.3 rename accordingly the LFC tables\n\n\n\n\n\n\n\nTip\n\n\n\nSee: configuring your own environment\n\n\n\n\nAll presentd data belongs to: 1. COVID-19 - proteomics - PhD Sonia Aparecida de Andrade at Escola Superior, Butantan Institute, S√£o Paulo, SP, Brazil 2. Medullobalstoma - microarray - PhD Aparecida Maria Fontes at Department of Genetics, Ribeir√£o Preto Medical School, University of S√£o Paulo, Ribeir√£o Preto"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Preface",
    "section": "",
    "text": "This book is free and open source, so please do open an issue if you notice a problem!"
  },
  {
    "objectID": "molecular_targets.html",
    "href": "molecular_targets.html",
    "title": "MTP",
    "section": "",
    "text": "The Molecular Target Pipeline (MTP)\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nAs Dr Wickham notes, this is Codd‚Äôs 3rd Normal Form but in statspeak rather than databasespeak.\nThe meaning of ‚Äúvariable‚Äù and ‚Äúobservation‚Äù depend on what we‚Äôre studying, so tidy data is a concept that you mostly learn through experience and ‚ú®vibes‚ú®.\nNow we‚Äôll explore what tidy data looks like for an NBA results dataset.\n\n\nYou can play around with the regressions yourself but we‚Äôll end them here.\n\n\n\nThis was mostly a demonstration of .pivot and .melt, with several different examples of reshaping data in Polars and Pandas."
  },
  {
    "objectID": "molecular_targets.html#get-the-data",
    "href": "molecular_targets.html#get-the-data",
    "title": "MTP",
    "section": "",
    "text": "You can play around with the regressions yourself but we‚Äôll end them here."
  },
  {
    "objectID": "molecular_targets.html#summary",
    "href": "molecular_targets.html#summary",
    "title": "MTP",
    "section": "",
    "text": "This was mostly a demonstration of .pivot and .melt, with several different examples of reshaping data in Polars and Pandas."
  },
  {
    "objectID": "BCA.html",
    "href": "BCA.html",
    "title": "BCA",
    "section": "",
    "text": "The Best Cutoff Algorithm (BCA)\n\n\nHere are some tips that are almost always a good idea:\n\nUse the lazy API.\nUse Exprs, and don‚Äôt use .apply unless you really have to.\nUse the smallest necessary numeric types (so if you have an integer between 0 and 255, use pl.UInt8, not pl.Int64). This will save both time and space.\nUse efficient storage (if you‚Äôre dumping stuff in files, Parquet is a good choice).\nUse categoricals for recurring strings (but note that it may not be worth it if there‚Äôs not much repetition).\nOnly select the columns you need.\n\n\n\n\n\n\n\nTip\n\n\n\nIf your colleagues are happy with CSVs and can‚Äôt be convinced to use something else, tell them that the Modern Polars book says they should feel bad.\n\n\nThese are basically the same rules you‚Äôd follow when using Pandas, except for the one about the lazy API. Now for some comparisons between the performance of idiomatic Pandas and Polars.\n\n\n\nHere we‚Äôll clean up a messy dataset, kindly provided by Kaggle user Rachit Toshniwal as a deliberate example of a really crap CSV. Most of the cleanup involves extracting numeric data from awkward strings.\nAlso, the data is too small so I‚Äôve concatenated it to itself 20 times. We‚Äôre not doing anything that will care about the duplication. Here‚Äôs how the raw table looks:\nFor this exercise we‚Äôll assume we want to make use of all the columns. First some boilerplate where we map out the different data types:\n:::\nThis example isn‚Äôt telling you to use Pandas in this specific situation. Once you add in the time spent reading a file, Polars likely wins.\nAnd even here, if you sort by the name col, Polars wins again. It has fast-track algorithms for sorted data.\n\n\n\n\nPolars is really fast. Pandas was already respectably fast and Polars wipes the floor with it.\nYou can still make Polars slow if you do silly things with it, but compared to Pandas it‚Äôs easier to do the right thing in the first place.\nPolars works well with NumPy ufuncs.\nThere are still some situations where Pandas can be faster. They are probably not compelling, but we shouldn‚Äôt pretend they don‚Äôt exist."
  },
  {
    "objectID": "BCA.html#six-fairly-obvious-performance-rules",
    "href": "BCA.html#six-fairly-obvious-performance-rules",
    "title": "BCA",
    "section": "",
    "text": "Here are some tips that are almost always a good idea:\n\nUse the lazy API.\nUse Exprs, and don‚Äôt use .apply unless you really have to.\nUse the smallest necessary numeric types (so if you have an integer between 0 and 255, use pl.UInt8, not pl.Int64). This will save both time and space.\nUse efficient storage (if you‚Äôre dumping stuff in files, Parquet is a good choice).\nUse categoricals for recurring strings (but note that it may not be worth it if there‚Äôs not much repetition).\nOnly select the columns you need.\n\n\n\n\n\n\n\nTip\n\n\n\nIf your colleagues are happy with CSVs and can‚Äôt be convinced to use something else, tell them that the Modern Polars book says they should feel bad.\n\n\nThese are basically the same rules you‚Äôd follow when using Pandas, except for the one about the lazy API. Now for some comparisons between the performance of idiomatic Pandas and Polars."
  },
  {
    "objectID": "BCA.html#polars-is-faster-at-the-boring-stuff",
    "href": "BCA.html#polars-is-faster-at-the-boring-stuff",
    "title": "BCA",
    "section": "",
    "text": "Here we‚Äôll clean up a messy dataset, kindly provided by Kaggle user Rachit Toshniwal as a deliberate example of a really crap CSV. Most of the cleanup involves extracting numeric data from awkward strings.\nAlso, the data is too small so I‚Äôve concatenated it to itself 20 times. We‚Äôre not doing anything that will care about the duplication. Here‚Äôs how the raw table looks:\nFor this exercise we‚Äôll assume we want to make use of all the columns. First some boilerplate where we map out the different data types:\n:::\nThis example isn‚Äôt telling you to use Pandas in this specific situation. Once you add in the time spent reading a file, Polars likely wins.\nAnd even here, if you sort by the name col, Polars wins again. It has fast-track algorithms for sorted data."
  },
  {
    "objectID": "BCA.html#summary",
    "href": "BCA.html#summary",
    "title": "BCA",
    "section": "",
    "text": "Polars is really fast. Pandas was already respectably fast and Polars wipes the floor with it.\nYou can still make Polars slow if you do silly things with it, but compared to Pandas it‚Äôs easier to do the right thing in the first place.\nPolars works well with NumPy ufuncs.\nThere are still some situations where Pandas can be faster. They are probably not compelling, but we shouldn‚Äôt pretend they don‚Äôt exist."
  },
  {
    "objectID": "DPC.html",
    "href": "DPC.html",
    "title": "DPC",
    "section": "",
    "text": "The Digital Pathway Curation (DPC) pipeline was designed and tested to answer the following questions:\n\nHow can we demonstrate that a molecular biological pathway is related to a disease?\n\nHow can we demonstrate whether a set of molecular biological pathways, calculated beyond the default cutoffs using GSEA, is related to a disease case?\n\nHow can someone query Gemini in a natural language and retrieve reproducible answers without hallucinations?\n\nHow can we quantify AI answers, make inferences, and calculate statistics?\n\nHow can we demonstrate that Gemini delivers reproducible results using a set of systems biology questions?\n\nRegarding the relationship between a set of molecular biological pathways and a disease, we also considered the following questions:\n\nHow can we calculate the accuracy of the answers provided by Gemini, PubMed, and humans?\n\nHow do we uncover FP and FN in a given enriched table?\n\nDPC was written in Python and integrated with the Gemini AI tool and PubMed through their web services. It stores an Ensemble of questions and answers to perform counts, comparisons and statistical analyses.\nIn this study, we defined and measured the concepts of consensus, reproducibility, crowdsourcing, and accuracy. Using a smaller dataset with two disease cases, we calculated crowdsource consensus (CSC) and assessed the accuracy of each of the ‚Äú3 Sources‚Äù: Gemini, PubMed, and human evaluations. Our findings indicate that Gemini‚Äôs consensus accuracy outperformed the PubMed and human accuracies. Additionally, using Gemini, we could uncover False Positive (FP) and False Negative (FN) pathways related to each disease case and confirm the True Positive (TP) and True Negative (TN) pathways.\n\n\n\n\n\n\nTip\n\n\n\nDPC aims to\n1. calculate Gemini (LLM) reproducibility\n2. calculate the consensus through the 4DSSQ\n3. merge the ‚Äú3 Sources‚Äù to calcualte the CSC\n4. calculates each source accuracy.\n\n\n\n\n\nA scientific study begins with a central question, and search engines like PubMed are the first tools for retrieving knowledge and understanding the current state of the art. Large Language Models (LLMs) have been used in research, promising acceleration and deeper results. However, besides caution, it demands rigorous validation. Assessing complex biological relationships, like those between molecular pathways and diseases, remains challenging for SQL-based tools and LLM models. Here, we introduce the Digital Pathway Curation (DPC) pipeline to systematically evaluate the reproducibility and accuracy of the Gemini models against PubMed search and human expert curation. Using the COVID-19 proteomic study for different cases and the Medulloblastoma (MB) transcriptomic study for different subtypes, we created a large dataset (Ensemble) based on determining pathway-disease associations. With the Ensemble dataset, we demonstrate that Gemini achieves high run-to-run reproducibility of approximately 99% and inter-model reproducibility of around 75%. Next, we calculate the crowdsourced consensus using a smaller dataset by integrating the ‚Äú3 Sources‚Äù: Gemini, PubMed and human evaluations. The CSC allows us to calculate each accuracy, and the Gemini multi-model consensus reached a significant accuracy of about 87%. Our findings demonstrate that LLMs are reproducible, reliable, and valuable tools for navigating complex biomedical knowledge using a well-formatted natural language query.\n\n\n\nThe scientific method - based on observation, hypothesis generation, feasible and reproducible experimental design, execution, data annotation, data analysis, hypothesis testing, and communication - relies on evidence and reproducibility. Moreover, the literature review is a crucial first step. Since most scientific literature stored in PubMed is published following peer review, one infers that all these articles follow the scientific method and are based on evidence, a statement questioned by some 8‚Äì12. Since PubMed is a reproducible SQL-based engine, the responses are identical once queried using the same search terms in a short interval between queries. However, PubMed‚Äôs accuracy is not as high as expected, particularly concerning false positives and negatives, which are often overlooked.\nIn the present study, Digital Pathway Curation confirms whether selected pathways are involved in modulating due to a disease. To establish a reproducibility measure, we propose using artificial intelligence, specifically the Gemini LLM models. The first step of our pipeline involves creating a large set of queries, called the Ensemble model. We can assess hard reproducibility by comparing the responses between runs or models. Additionally, we introduce a consensus method, which identifies the most frequently chosen answer among ‚Äúfour different but semantically similar questions‚Äù (4DSSQ). Using this consensus, we propose a measure of soft reproducibility, which involves comparing the results from different runs or models based on the consensus results.\nNext, we propose a smaller dataset called ‚Äú3 Sources‚Äù dataset (3SD), based on a list of ‚Äútwo cases of randomly selected pathways‚Äù (2CRSP), containing randomly selected pathways for each case. This dataset includes a list of 30 pathways that Gemini, PubMed, and Human respond to related to each disease case or subtype. With 3SD results, we can calculate the crowdsourced consensus score (CSS25) and assess the accuracy of each source.\nThe DPC pipeline evaluates the ability of LLMs to determine whether selected pathways are involved in modulating due to a disease, utilising GSEA results based on the Reactome database. Given an omic study, DPC selects calculated pathways stored in the GSEA pathway table using the ‚Äúfour pathway groups‚Äù method. These pathways are then used to perform the Ensemble dataset analyses.\nThe Ensemble dataset comprises the four pathway groups containing tens or hundreds of pathways. It incorporates at least two Gemini models and four distinct semantic forms of questions posed at different times, called runs. This dataset allows for evaluating the run-to-run, inter-model, and semantic reproducibilities. To analyse the semantic reproducibility, we introduced the concept of the ‚Äúfour different and semantically similar questions‚Äù (4DSSQ). Additionally, Tiwari et al.¬†from the EMBL-EBI developed a similar approach to test the Reactome database completion using ChatGPT24. We also compared the Ensemble answers with those obtained from PubMed searches.\nThe CSC allows us to assess the accuracy of each source. Using the source with the best accuracy according to the CSC, we can uncover False Positives (FP) and False Negatives (FN) while also confirming True Positives (TP) and True Negatives (TN) among the enriched pathways calculated with GSEA.\nIn the upcoming sections, you will find a detailed description of each method and the results obtained.\n\n\n\nDaniel Alexandre de Souza2, Carlos Eduardo Madureira Trufen5, Victor Wendel da Silva Gon√ßalves3, Juliana de Paula Bernardes3, Vin√≠cius Miranda Baroni3, Carlos DeOcesano-Pereira1, Leonardo Fontoura Ormundo8, Fabio Augusto Labre de Souza3,4, Olga Celia Martinez Iba√±ez6, Nancy Starobinas6, Luciano Rodrigo Lopes7, Aparecida Maria Fontes3, Sonia Aparecida de Andrade9, Ana Marisa Chudzinski-Tavassi1,2\n1 - CENTD, Centre of Excellence in New Target Discovery, Butantan Institute, S√£o Paulo, Brazil.\n2 - Laboratory of Development and Innovation, Butantan Institute, S√£o Paulo, SP, Brazil.\n3 - Department of Genetics, Ribeir√£o Preto Medical School, University of S√£o Paulo, Ribeir√£o Preto 14049-900, SP, Brazil\n4- Department of Medical Imaging, Hematology, and Clinical Oncology, Ribeir√£o Preto Medical School, University of S√£o Paulo, Ribeir√£o Preto 14049-900, SP, Brazil\n5 - Immunochemistry Laboratory, Prevor, Li√®ge, Belgium.\n6 - Immunogenetics Laboratory, Butantan Institute, S√£o Paulo, Brazil.\n7 - Department of Health Informatics, UNIFESP, S√£o Paulo, Brazil.\n8 - Medical Investigation Laboratory 60, School of Medicine, University of S√£o Paulo, S√£o Paulo, Brazil.\n9. Escola Superior do Instituto Butantan, Butantan Institute, S√£o Paulo, SP, Brazil.\n\n\n\nArtificial Intelligence, Large Language Model, PubMed search, Google, Gemini, Reproducibility, Accuracy, Curation, Biomedical Pathways, Bioinformatics, Semantic Queries"
  },
  {
    "objectID": "DPC.html#why",
    "href": "DPC.html#why",
    "title": "DPC",
    "section": "",
    "text": "The Digital Pathway Curation (DPC) pipeline was designed and tested to answer the following questions:\n\nHow can we demonstrate that a molecular biological pathway is related to a disease?\n\nHow can we demonstrate whether a set of molecular biological pathways, calculated beyond the default cutoffs using GSEA, is related to a disease case?\n\nHow can someone query Gemini in a natural language and retrieve reproducible answers without hallucinations?\n\nHow can we quantify AI answers, make inferences, and calculate statistics?\n\nHow can we demonstrate that Gemini delivers reproducible results using a set of systems biology questions?\n\nRegarding the relationship between a set of molecular biological pathways and a disease, we also considered the following questions:\n\nHow can we calculate the accuracy of the answers provided by Gemini, PubMed, and humans?\n\nHow do we uncover FP and FN in a given enriched table?\n\nDPC was written in Python and integrated with the Gemini AI tool and PubMed through their web services. It stores an Ensemble of questions and answers to perform counts, comparisons and statistical analyses.\nIn this study, we defined and measured the concepts of consensus, reproducibility, crowdsourcing, and accuracy. Using a smaller dataset with two disease cases, we calculated crowdsource consensus (CSC) and assessed the accuracy of each of the ‚Äú3 Sources‚Äù: Gemini, PubMed, and human evaluations. Our findings indicate that Gemini‚Äôs consensus accuracy outperformed the PubMed and human accuracies. Additionally, using Gemini, we could uncover False Positive (FP) and False Negative (FN) pathways related to each disease case and confirm the True Positive (TP) and True Negative (TN) pathways.\n\n\n\n\n\n\nTip\n\n\n\nDPC aims to\n1. calculate Gemini (LLM) reproducibility\n2. calculate the consensus through the 4DSSQ\n3. merge the ‚Äú3 Sources‚Äù to calcualte the CSC\n4. calculates each source accuracy."
  },
  {
    "objectID": "DPC.html#abstract",
    "href": "DPC.html#abstract",
    "title": "DPC",
    "section": "",
    "text": "A scientific study begins with a central question, and search engines like PubMed are the first tools for retrieving knowledge and understanding the current state of the art. Large Language Models (LLMs) have been used in research, promising acceleration and deeper results. However, besides caution, it demands rigorous validation. Assessing complex biological relationships, like those between molecular pathways and diseases, remains challenging for SQL-based tools and LLM models. Here, we introduce the Digital Pathway Curation (DPC) pipeline to systematically evaluate the reproducibility and accuracy of the Gemini models against PubMed search and human expert curation. Using the COVID-19 proteomic study for different cases and the Medulloblastoma (MB) transcriptomic study for different subtypes, we created a large dataset (Ensemble) based on determining pathway-disease associations. With the Ensemble dataset, we demonstrate that Gemini achieves high run-to-run reproducibility of approximately 99% and inter-model reproducibility of around 75%. Next, we calculate the crowdsourced consensus using a smaller dataset by integrating the ‚Äú3 Sources‚Äù: Gemini, PubMed and human evaluations. The CSC allows us to calculate each accuracy, and the Gemini multi-model consensus reached a significant accuracy of about 87%. Our findings demonstrate that LLMs are reproducible, reliable, and valuable tools for navigating complex biomedical knowledge using a well-formatted natural language query."
  },
  {
    "objectID": "DPC.html#introduction",
    "href": "DPC.html#introduction",
    "title": "DPC",
    "section": "",
    "text": "The scientific method - based on observation, hypothesis generation, feasible and reproducible experimental design, execution, data annotation, data analysis, hypothesis testing, and communication - relies on evidence and reproducibility. Moreover, the literature review is a crucial first step. Since most scientific literature stored in PubMed is published following peer review, one infers that all these articles follow the scientific method and are based on evidence, a statement questioned by some 8‚Äì12. Since PubMed is a reproducible SQL-based engine, the responses are identical once queried using the same search terms in a short interval between queries. However, PubMed‚Äôs accuracy is not as high as expected, particularly concerning false positives and negatives, which are often overlooked.\nIn the present study, Digital Pathway Curation confirms whether selected pathways are involved in modulating due to a disease. To establish a reproducibility measure, we propose using artificial intelligence, specifically the Gemini LLM models. The first step of our pipeline involves creating a large set of queries, called the Ensemble model. We can assess hard reproducibility by comparing the responses between runs or models. Additionally, we introduce a consensus method, which identifies the most frequently chosen answer among ‚Äúfour different but semantically similar questions‚Äù (4DSSQ). Using this consensus, we propose a measure of soft reproducibility, which involves comparing the results from different runs or models based on the consensus results.\nNext, we propose a smaller dataset called ‚Äú3 Sources‚Äù dataset (3SD), based on a list of ‚Äútwo cases of randomly selected pathways‚Äù (2CRSP), containing randomly selected pathways for each case. This dataset includes a list of 30 pathways that Gemini, PubMed, and Human respond to related to each disease case or subtype. With 3SD results, we can calculate the crowdsourced consensus score (CSS25) and assess the accuracy of each source.\nThe DPC pipeline evaluates the ability of LLMs to determine whether selected pathways are involved in modulating due to a disease, utilising GSEA results based on the Reactome database. Given an omic study, DPC selects calculated pathways stored in the GSEA pathway table using the ‚Äúfour pathway groups‚Äù method. These pathways are then used to perform the Ensemble dataset analyses.\nThe Ensemble dataset comprises the four pathway groups containing tens or hundreds of pathways. It incorporates at least two Gemini models and four distinct semantic forms of questions posed at different times, called runs. This dataset allows for evaluating the run-to-run, inter-model, and semantic reproducibilities. To analyse the semantic reproducibility, we introduced the concept of the ‚Äúfour different and semantically similar questions‚Äù (4DSSQ). Additionally, Tiwari et al.¬†from the EMBL-EBI developed a similar approach to test the Reactome database completion using ChatGPT24. We also compared the Ensemble answers with those obtained from PubMed searches.\nThe CSC allows us to assess the accuracy of each source. Using the source with the best accuracy according to the CSC, we can uncover False Positives (FP) and False Negatives (FN) while also confirming True Positives (TP) and True Negatives (TN) among the enriched pathways calculated with GSEA.\nIn the upcoming sections, you will find a detailed description of each method and the results obtained."
  },
  {
    "objectID": "DPC.html#collaborations",
    "href": "DPC.html#collaborations",
    "title": "DPC",
    "section": "",
    "text": "Daniel Alexandre de Souza2, Carlos Eduardo Madureira Trufen5, Victor Wendel da Silva Gon√ßalves3, Juliana de Paula Bernardes3, Vin√≠cius Miranda Baroni3, Carlos DeOcesano-Pereira1, Leonardo Fontoura Ormundo8, Fabio Augusto Labre de Souza3,4, Olga Celia Martinez Iba√±ez6, Nancy Starobinas6, Luciano Rodrigo Lopes7, Aparecida Maria Fontes3, Sonia Aparecida de Andrade9, Ana Marisa Chudzinski-Tavassi1,2\n1 - CENTD, Centre of Excellence in New Target Discovery, Butantan Institute, S√£o Paulo, Brazil.\n2 - Laboratory of Development and Innovation, Butantan Institute, S√£o Paulo, SP, Brazil.\n3 - Department of Genetics, Ribeir√£o Preto Medical School, University of S√£o Paulo, Ribeir√£o Preto 14049-900, SP, Brazil\n4- Department of Medical Imaging, Hematology, and Clinical Oncology, Ribeir√£o Preto Medical School, University of S√£o Paulo, Ribeir√£o Preto 14049-900, SP, Brazil\n5 - Immunochemistry Laboratory, Prevor, Li√®ge, Belgium.\n6 - Immunogenetics Laboratory, Butantan Institute, S√£o Paulo, Brazil.\n7 - Department of Health Informatics, UNIFESP, S√£o Paulo, Brazil.\n8 - Medical Investigation Laboratory 60, School of Medicine, University of S√£o Paulo, S√£o Paulo, Brazil.\n9. Escola Superior do Instituto Butantan, Butantan Institute, S√£o Paulo, SP, Brazil."
  },
  {
    "objectID": "DPC.html#keywords",
    "href": "DPC.html#keywords",
    "title": "DPC",
    "section": "",
    "text": "Artificial Intelligence, Large Language Model, PubMed search, Google, Gemini, Reproducibility, Accuracy, Curation, Biomedical Pathways, Bioinformatics, Semantic Queries"
  },
  {
    "objectID": "concepts.html",
    "href": "concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Many people don‚Äôt need to be told why method chaining is nice. Many languages make it easy to write thing.min().abs().str() instead of str(abs(min(thing))).\nBut the Python standard library tends to prefer the ugly way, so if you‚Äôve spent a lot of time doing vanilla Python stuff, you may have become accustomed to writing nasty nested function calls or declaring intermediate variables like min_thing. This is sad because a sensible degree of method chaining can make code a lot easier to read.\nIt‚Äôs not always easy for libraries to accommodate method chaining - in fancy terms, to be fluent interfaces. Even Pandas used to be much less fluent: when Modern Pandas was released, methods like assign and pipe were quite recent.\nFortunately Polars is very fluent. The expression API provides a very elegant way to do a bunch of stuff to a dataframe in one fell swoop, and Polars mostly doesn‚Äôt mutate dataframes in-place (method chaining with side effects is often a bad idea).\nLet‚Äôs see how this looks in practice by cleaning up the flight data we gathered in ?@sec-indexing.\n:::\nSome items to note:\n\nOur Pandas function adds columns to a dataframe, while our Polars function simply generates a Polars expression. You‚Äôll find it‚Äôs often easier to pass around Exprs than dataframes because:\n\nThey work on both DataFrame and LazyFrame, and they aren‚Äôt bound to any particular data.\nPolars performs better if you put everything in one .select or .with_columns call, rather than calling .select multiple times. If you pass around expressions, this pattern is easy.\n\nPolars is fast and convenient for doing the same thing to multiple columns. We can pass a list of columns to pl.col and then call a method on that pl.col as if it were one column. When the expression gets executed it will be parallelized by Polars.\nMeanwhile in Pandas we have to loop through the columns to create a dictionary of kwargs for .assign. This is not parallelized. (We could use .apply with axis=0 instead but this would still take place sequentially and isn‚Äôt any easier to read, in my opinion).\nCalling .str.split in Polars creates a column where every element is a list. This kind of data is annoying in Pandas because it‚Äôs slow and awkward to work with - notice how the most convenient way to get the first element of a list column in Pandas is to call .str[0], even though this is a list, not a string ü§î\nI‚Äôm not sure if that‚Äôs even supposed to work. In contrast, Polars actually has first class support for list columns, and they are fast as long as they don‚Äôt have mixed types.\n\n\nPolars\n\n\n\n\n\n\n\n\nThe above examples have some fairly long method chains that could perhaps be split up. That said, if I saw them in a PR I probably wouldn‚Äôt mind. Here‚Äôs a chain that‚Äôs too long:\n\nI would argue this code is not hard to follow, but empirically it provokes disgust in people. I think it‚Äôs just visually shocking, which is often a hallmark of terrible code. That‚Äôs enough reason to avoid doing this. You don‚Äôt want folks assuming you‚Äôve lost your mind.\nIn the above example, it would probably be enough to just move those long lists to their own variables. That would make it more obvious that the code is really doing simple, menial things.\n\n\n\n\nMethod chaining is great but use it with care.\nThe Polars code looks pretty good here next to Pandas. It‚Äôs less clunky and more conducive to fluent programming. I‚Äôve noticed some people are under the impression that you should only use Polars when you need the performance, but I would argue the above examples show that it‚Äôs better for small data too.\nIf you want to translate Pandas code to Polars, pay attention to the examples above and the accompanying notes on the API differences."
  },
  {
    "objectID": "concepts.html#how-much-is-too-much",
    "href": "concepts.html#how-much-is-too-much",
    "title": "Concepts",
    "section": "",
    "text": "The above examples have some fairly long method chains that could perhaps be split up. That said, if I saw them in a PR I probably wouldn‚Äôt mind. Here‚Äôs a chain that‚Äôs too long:\n\nI would argue this code is not hard to follow, but empirically it provokes disgust in people. I think it‚Äôs just visually shocking, which is often a hallmark of terrible code. That‚Äôs enough reason to avoid doing this. You don‚Äôt want folks assuming you‚Äôve lost your mind.\nIn the above example, it would probably be enough to just move those long lists to their own variables. That would make it more obvious that the code is really doing simple, menial things."
  },
  {
    "objectID": "concepts.html#summary",
    "href": "concepts.html#summary",
    "title": "Concepts",
    "section": "",
    "text": "Method chaining is great but use it with care.\nThe Polars code looks pretty good here next to Pandas. It‚Äôs less clunky and more conducive to fluent programming. I‚Äôve noticed some people are under the impression that you should only use Polars when you need the performance, but I would argue the above examples show that it‚Äôs better for small data too.\nIf you want to translate Pandas code to Polars, pay attention to the examples above and the accompanying notes on the API differences."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "pPMod.html",
    "href": "pPMod.html",
    "title": "pPMod",
    "section": "",
    "text": "pseudo-Pathway modulation (pPMod) is the third module ‚Ä¶ this chapter we‚Äôll mostly compare Polars to Dask rather than to Pandas. This isn‚Äôt an apples-to-apples comparison, because Dask helps scale Pandas but it might help scale Polars too one day. Dask, like Spark, can run on a single node or on a cluster with thousands of nodes.\nPolars doesn‚Äôt come with any tooling for running on a cluster, but it does have a streaming mode for larger-than-memory datasets on a single machine. It also uses memory more efficiently than Pandas. These two things mean you can use Polars for much bigger data than Pandas can handle, and hopefully you won‚Äôt need tools like Dask or Spark until you‚Äôre actually running on a cluster.\n\n\n\n\n\n\nNote\n\n\n\nI use ‚ÄúDask‚Äù here as a shorthand for dask.dataframe. Dask does a bunch of other stuff too.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe streaming features of Polars are very new at the time of writing, so approach with caution!\n\n\n\n\nWe‚Äôll be using political donation data from the FEC. Warning: this takes a few minutes.\n:::"
  },
  {
    "objectID": "pPMod.html#get-the-data",
    "href": "pPMod.html#get-the-data",
    "title": "pPMod",
    "section": "",
    "text": "We‚Äôll be using political donation data from the FEC. Warning: this takes a few minutes.\n:::"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming."
  }
]